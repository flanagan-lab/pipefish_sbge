---
title: "Weighted Gene Co-expression Network Analysis (WGCNA) in ANZ and Aus _Stigmatopora nigra_"
output:
  bookdown::pdf_document2:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
    toc: true
header-includes: >
  \usepackage{lipsum}
  \usepackage{float}
  \floatplacement{figure}{H}
editor_options:
  chunk_output_type: console
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir='../',fig_path="../imgs/")
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)
```

``` {r library, include = TRUE, message = FALSE, warning = FALSE}
setwd("~/Documents/GitHub/pipefish_sbge/stigmatopora/wgcna")
#This is a cohesive list of all the libraries used in this document
library(DESeq2)
library(flashClust)
library(tidyverse)
library(pheatmap)
library(dplyr)
library(readr)
library(WGCNA)
library(foreach)
library(doParallel)
library(matrixStats)
```

## Using OrthoFinder in the RCC

Because the transcriptomes were assembled de novo and have no gene annotation, we'll need to use OrthoFinder to be able to meaningfully compare the two populations.

To run OrthoFinder, you need protein sequences. To achieve this, use TransDecoder:

```{bash, transdecoder, eval = FALSE}

# For each transcriptome FASTA
TransDecoder.LongOrfs -t species1_transcripts.fasta 
TransDecoder.Predict -t species1_transcripts.fasta
```

Running `./TransDecoder.LongOrfs` doesn't take long - no more than 10 minutes on the RCC.
For NZ nigra this was run as: `./TransDecoder.LongOrfs -t ../emily_files/S_nigra_2024/nigra_supertranscript.fasta`
For Aus nigra this was run as: `./TransDecoder.LongOrfs -t ../emily_files/australia_nigra_raw_reads/supertranscriptome.fasta`

and

For NZ nigra this was run as: `./TransDecoder.Predict -t ../emily_files/S_nigra_2024/nigra_supertranscript.fasta`
     redundancy-minimized set includes 4870 / 5000 = 97.40%
     
For Aus nigra this was run as: `./TransDecoder.Predict -t ../emily_files/australia_nigra_raw_reads/supertranscriptome.fasta`
     redundancy-minimized set includes 4978 / 5000 = 99.56%

Running `./TransDecoder.Predict` takes much longer - depending on the size of your transcriptome, it could take several hours. Run with nohup or in a screen session.  
Running this code gives you a `.pep` file for each - your protein sequences.


Then you have to prepare the input for OrthoFinder by creating a directory like this:

```{bash, prepare-input, eval = FALSE}
orthofinder_input/
  nz_nigra.fa     # <- from TransDecoder
  aus_nigra.fa     # <- from TransDecoder
```

Then you can run OrthoFinder

```{bash, run-orthofinder, eval = FALSE}

orthofinder -f orthofinder_input/ -t 8

```

This was run as `./orthofinder -f ../TransDecoder-TransDecoder-v5.7.1/orthofinder_input/`

Running this command will: 1) Run DIAMOND to align proteins, 2) cluster transcripts into orthogroups, 3) Infer orthologs and gene trees. 

Then you can use the orthogroups.tsv to merge with expression data. This will allow you to summarise expression values by orthogroup and build a shared expression matrix for WGCNA. 

Note - CITATION:
 When publishing work that uses OrthoFinder please cite:
 Emms D.M. & Kelly S. (2019), Genome Biology 20:238

 If you use the species tree in your work then please also cite:
 Emms D.M. & Kelly S. (2017), MBE 34(12): 3267-3278
 Emms D.M. & Kelly S. (2018), bioRxiv https://doi.org/10.1101/267914

## Get expression data for NZ _S. nigra_

``` {r read-data}
#The abundance matrix generated via salmon and tximport to be used for the DE analysis
txi.nigra <- readRDS("data/txi.salmon_SN.RDS")

#The samples file generated for tximport
samples <- read.table("data/SN_samples.txt", header = TRUE)

#Make sure the conditions are in the samples file as a factor
samples$sex <- as.factor(samples$sex)
samples$organ <- as.factor(samples$organ)

```

```{r DESeqDataSet, message=FALSE, warning=FALSE}
#Create the DESeq dataset
dds_SN <- DESeqDataSetFromTximport(txi.nigra, 
                                   colData = samples,
                                   design = ~ sex)

```

The data is then pre-filtered to remove low gene counts before running further DESeq2 functions. By doing this we remove rows in which there are very few reads thus reducing the memory size of the `dds` object and increasing the speed at which we can use the transformation and testing functions in DESeq2.

The cutoff here was to remove rows that had counts fewer than 10 across all samples.

```{r pre-filtering, message=FALSE, warning=FALSE}
#only keeping rows that have at least 10 reads total
keep <- rowSums(counts(dds_SN)) >= 10
dds_SN <- dds_SN[keep, ]

dds_SN <- dds_SN[, !(dds_SN$ID %in% c("S34", "S41"))]
samples <- samples[!(samples$ID %in% c ("S34", "S41")),]
```

After filtering we can now perform the standard differential expression analysis that is wrapped into DESeq2.

```{r diff-exp, message=FALSE, warning=FALSE}
#Generate the expression values
dds_SN_exp <- DESeq(dds_SN)

#Compile the results
res <- results(dds_SN_exp)
res

```

## Get expression data for Aus _S. nigra_

``` {r read-data}
#The abundance matrix generated via salmon and tximport to be used for the DE analysis
txi.aus <- readRDS("data/txi.salmon_aus.RDS")

#The samples file generated for tximport
samples <- read.table("data/aus_samples.txt", header = TRUE)

#Make sure the conditions are in the samples file as a factor
samples$sex <- as.factor(samples$sex)
samples$organ <- as.factor(samples$organ)

```

The package `DESeq2` was used for the differential expression analysis outlined below.

# Single factor analysis - Comparing Males v Females across all organs
To analyze your data with DESeq2 you must first generate the DESeqDataSet. In order to do this we need the abundance matrix generated with `tximport` and a `samples` file that lays out all of the conditions. The model for this single factor analysis was run as counts ~ Sex.

```{r DESeqDataSet, message=FALSE, warning=FALSE}
#Create the DESeq dataset
dds_aus <- DESeqDataSetFromTximport(txi.aus, 
                                   colData = samples,
                                   design = ~ sex)

dds_aus <- dds_aus[, !(dds_aus$ID %in% c("S3","S10","S13","S16"))]

samples <- samples[!(dds_aus$ID %in% c("S3","S10","S13","S16")), ]
```

The data is then pre-filtered to remove low gene counts before running further DESeq2 functions. By doing this we remove rows in which there are very few reads thus reducing the memory size of the `dds` object and increasing the speed at which we can use the transformation and testing functions in DESeq2.

The cutoff here was to remove rows that had counts fewer than 10 across all samples.

```{r pre-filtering, message=FALSE, warning=FALSE}
#only keeping rows that have at least 10 reads total
keep <- rowSums(counts(dds_aus)) >= 10
dds_aus <- dds_aus[keep, ]

```

After filtering we can now perform the standard differential expression analysis that is wrapped into DESeq2.

```{r diff-exp, message=FALSE, warning=FALSE}
#Generate the expression values
dds_aus_exp <- DESeq(dds_aus)

#Compile the results
res <- results(dds_aus_exp)
res

```

For WGCNA, we need approximately homoscedastic, normalised data, so we need to apply variance stabilising transformation.

```{r VST}
vsd_SN <- vst(dds_SN_exp, blind = TRUE)
vsd_aus <- vst(dds_aus_exp, blind = TRUE)
expr_SN <- assay(vsd_SN)
expr_aus <- assay(vsd_aus)
```

# Read in OrthoFinder input

```{r read-filter-Orthogroup}
## Load and prepare orthology data
orthogroups <- read.delim("data/Orthogroups.tsv", header = TRUE, stringsAsFactors = FALSE, fill = TRUE)

```

