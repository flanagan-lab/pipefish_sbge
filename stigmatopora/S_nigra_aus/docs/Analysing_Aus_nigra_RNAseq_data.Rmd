---
title: "Analysing Australian *Stigmatopora nigra* RNAseq Data"
author: "Emily Beasley"
date: "`r Sys.Date()`"
output: 
    html_document:
        code_folding: show
        toc: yes
        toc_float: yes
        number_sections: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir='../',fig_path="../imgs/")
```

``` {r library, include = FALSE}
#library(tximport) #This was used to generate an abundance matrix from the salmon quant outputs
```
This pipeline follows a pipeline outlined for _Syngnathus floridae_. Changes in scripts and outputs are outlined in this document. More detail about the programmes used can be found here:https://github.com/coley-tosto/PhD_thesis/blob/main/Chapter_3/docs/Analyzing_floridae_RNAseq_data_from_MSU.Rmd This pipeline was run on the University of Canterbury's RCC.

# Pre-assembly Quality Control and Filtering

## Trimming the Reads with Trimmomatic
[Trimmomatic](http://www.usadellab.org/cms/index.php?page=trimmomatic) is commonly used for Illumina paired-end and single ended data.  

### Running trimmomatic across Multiple Pairs of Reads

Trimmomatic was installed on the RCC through conda in a conda environment called **Trim**. Use command `conda activate Trim` before running Trimmomatic. 


```{bash}
#!/bin/bash

#Create the argument
input_dir=$1 #This should be the location of the RAW reads to be trimmed

for fq in $1*_R1_001.fastq.gz
        do
        base=$(basename $fq _R1_001.fastq.gz)
        echo "Running trimmomatic for ${base}..."
        time trimmomatic PE -threads 16 $fq $1${base}_R2_001.fastq.gz \
                $1trimmed/${base}_paired_R1_001.fastq.gz $1trimmed/${base}_unpaired_R1_001.fastq.gz \
                $1trimmed/${base}_paired_R2_001.fastq.gz $1trimmed/${base}_unpaired__R2_001.fastq.gz \
                HEADCROP:12 LEADING:3 TRAILING:10 SLIDINGWINDOW:4:15 MINLEN:50
done

```
-   Adapter content was removed prior to receiving the raw reads from Micromon, so the bash script `trim_no_adapter_run.sh` was used. If adapter content is present, use bash script `trimmomatic_run.sh`

-   Before running the script, change the `trimmomatic` line to `echo "..."` to make sure all of the variables are working correctly.

-   Trimmomatic takes about a day to run, so either run it with nohup or screen 

-   Then remove the `echo "..."` and run the script as `nohup bash ../scripts/trim_no_adapter_run.sh ../australia_nigra_raw_reads/ > trim.out 2>&1 &`.

## Using SortMeRNA to remove rRNA contamination
### Create a new environment and install SortMeRNA in it

SortMeRNA was installed in a conda environment as follows:

```
conda create -n sortmerna
conda activate sortmerna
conda install sortmerna

which sortmerna #check location of installation
  /home/rccuser/anaconda3/envs/contamination/bin/sortmerna
```

### Test the installation of SortMeRNA

```
sortmerna --version
    SortMeRNA version 4.3.6
    Build Date: Aug 16 2022
    sortmerna_build_git_sha:@db8c1983765f61986b46ee686734749eda235dcc@
    sortmerna_build_git_date:@2022/08/16 11:42:59@
```

### Download the latest SortMeRNA databases (v4.3.4)
The `database.tar.gz` file can also be downloaded from [this link](https://github.com/sortmerna/sortmerna/releases).

```
wget https://github.com/biocore/sortmerna/releases/download/v4.3.4/database.tar.gz
mkdir rRNA_databases_v4.3.4
tar -xvf database.tar.gz -C rRNA_databases_v4.3.4
```

### Run SortMeRNA - creating a for loop to run multiple samples at a time
```{bash, eval = FALSE}
#!/bin/bash

#Create arguments
input_dir=$1 #location of the reads
ref_fasta=$2 #desired reference fasta
output_dir_rrna=$3 #desired location for the reads that are rRNA
output_dir_norrna=$4 #desired location for the reads that are NOT rRNA

## Loop through all pairs of reads in the input directory
for pair in $1/*_R1_001.fastq.gz
        do

        #Extract the sample name from the file name
        sample=$(basename $pair _R1_001.fastq.gz)

        #Extract Fish ID
        ID=$(basename $pair _paired_R1_001.fastq.gz)

        ##Echo the sample name it is currently running
        echo "Running SortMeRNA for ${ID}..."

        #Define the paths to the input and output files for this sample
        input1=$1/${sample}_R1_001.fastq.gz
        input2=$1/${sample}_R2_001.fastq.gz

        #Run SortMeRNA on this pair of reads
        time sortmerna --threads 16 --ref $2 --reads $input1 --reads $input2 --fastx --aligned $3/${ID} --other $4/${ID} --out2

        #Remove SortMeRNA intermediate files before running again
        rm -r /home/rccuser/sortmerna/run/kvdb

done


```

This script was run as `nohup bash ../../scripts/sortmerna_run.sh paired/ /home/rccuser/shared/rRNA_databases_v4.3.4/smr_v4.3_fast_db.fasta rrna/ no_rrna/ > sortmerna.log 2>&1 &`

SortMeRNA takes 1-3 days to run depending on the number of samples you have. It took 2 days for these 16 paired samples.  

## Using Kraken2 to remove biological contamination

Kraken2 was installed in the conda environment `kraken2`

### Creating a for loop to run multiple samples at a time
```{bash, eval = FALSE}

#!/bin/bash

#Create arguments
ref_fasta=$1 #desired reference database
input_dir=$2 #Input directory with the location of the reads
output_dir=$3 #Desired location of the output

## Loop through all pairs of reads in the input directory
for pair in $2/*_fwd.fq.gz
        do

        #Extract the sample name from the file name
        sample=$(basename $pair _fwd.fq.gz)

        ##Echo the sample name it is currently running
        echo "Running Kraken2 for ${sample}..."

        #Define the paths to the input and output files for this sample
        input1=$2/${sample}_fwd.fq.gz
        input2=$2/${sample}_rev.fq.gz

        #Run Kraken2 on this pair of reads
        time kraken2 --threads 16 --db $1 --paired $input1 $input2 --unclassified-out $3/${sample}#.fq --report $3/${sample}.log

done

```


This script was run as `nohup bash ../../scripts/kraken2_run.sh /home/rccuser/shared/kraken2_pluspfp/ no_rrna/ kraken_output/ > kraken2.log 2>&1 &`

## Doing a k-mer based correction with RCorrector
Rcorrector (RNA-seq error CORRECTOR) is a kmer-based error correction method that is used on RNA-seq data. This program allows for a correction of random sequencing errors in Illumina RNA-seq reads. The downside is you may lose information on things like SNPs and other sequence variants.


### Creating a for loop to run multiple samples at a time
```{bash, eval = FALSE}
### Creating a for loop to run multiple samples at a time
#!/bin/bash

#Create arguments
rcorrector_path=$1 #Path to the run_rcorrector.pl file
input_dir=$2 #Path to the input directory containing the reads
output_dir=$3 #Path to the desired output location

##Loop through all pairs of reads in the directory
for pair in $2/*_1.fq
        do

        #Extract sample name from the file name
        sample=$(basename $pair _1.fq)

        #Echo the sample name that is currently running
        echo "Running rcorrector for ${sample} ..."

        #Run rcorrector on this pair of reads
        time perl $1/run_rcorrector.pl -t 16 -1 $2/${sample}_1.fq -2 $2/${sample}_2.fq -od $3

done

```

This script was run as `nohup bash ../../scripts/rcorrector_run.sh /home/rccuser/rcorrector/ kraken_output/ rcorrector/ > rcor.log 2>&1 &`

# Checking quality of trimmed and filtered reads
The quality of the reads once they finished going through the filtering and trimming pipeline outlined above was assessed with FastQC and the results were compiled using MultiQC.

1. **Trim** environment: Here is where the programs **Trimmomatic** and **fastqc** were installed.

2. **QC** environment: This is where **multiqc** was installed.

```{bash, eval = FALSE}
#!/bin/bash

input_dir=$1 #location of the reads
output_dir=$2 #name of the desired output directory

for fq in $1/*.cor.fq
        do
        base=$(basename $fq)
        echo "Running fastqc for ${base} ..."
        time fastqc $fq -t 16 -o $2
done

```

This script was run as `nohup bash ../../scripts/fastqc_run.sh rcorrector/ fastqc/ > fastqc.log 2>&1 &`. Once finished MultiQC was run: `multiqc fastqc`. 

# De novo transcriptome assembly

## Installing Trinity
Trinity requires the installation of several programs, to get around this Trinity was installed via a Docker in the RCC and must be used within one. **The first time that you use Trinity you must pull the latest Docker image for Trinity** like so:

```
docker pull trinityrnaseq/trinityrnaseq

```

## Running Trinity 
Once started, a successful Trinity run will take days to complete. There are a lot of extra components you can add onto a Trinity run (View the [Trinity User Manual](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running-Trinity) for more information).
    
### Using multiple reads
#### Creating a Sample Name File
There are two options when using multiple files to generate your de novo transcriptome assembly. Each sample can be typed out individually, ensuring that the two reads are paired up by order. Alternatively, the argument `--samples_file` can be used instead. With this argument you must provide a tab-delimited text file indicating biological replicate relationships. This is the example provided on the [Trinity Website](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running-Trinity):

         cond_A    cond_A_rep1    A_rep1_left.fq    A_rep1_right.fq
         cond_A    cond_A_rep2    A_rep2_left.fq    A_rep2_right.fq
         cond_B    cond_B_rep1    B_rep1_left.fq    B_rep1_right.fq
         cond_B    cond_B_rep2    B_rep2_left.fq    B_rep2_right.fq
         
The samples file that was provided for the script below was adapted into this format:

```
SNF11G	S1	/home/rccuser/shared/emily_files/australia_nigra_raw_reads/trimmed/rcorrector/SNF11G_S1_L001_1.cor.fq	/home/rccuser/shared/emily_files/australia_nigra_raw_reads/trimmed/rcorrector/SNF11G_S1_L001_2.cor.fq
SNF11L	S2	/home/rccuser/shared/emily_files/australia_nigra_raw_reads/trimmed/rcorrector/SNF11L_S2_L001_1.cor.fq	/home/rccuser/shared/emily_files/australia_nigra_raw_reads/trimmed/rcorrector/SNF11L_S2_L001_2.cor.fq
SNF11O	S3	/home/rccuser/shared/emily_files/australia_nigra_raw_reads/trimmed/rcorrector/SNF11O_S3_L001_1.cor.fq /home/rccuser/shared/emily_files/australia_nigra_raw_reads/trimmed/rcorrector/SNF11O_S3_L001_2.cor.fq
SNF12G	S4	/home/rccuser/shared/emily_files/australia_nigra_raw_reads/trimmed/rcorrector/SNF12G_S4_L001_1.cor.fq	/home/rccuser/shared/emily_files/australia_nigra_raw_reads/trimmed/rcorrector/SNF12G_S4_L001_2.cor.fq
SNF12L	S5	/home/rccuser/shared/emily_files/australia_nigra_raw_reads/trimmed/rcorrector/SNF12L_S5_L001_1.cor.fq	/home/rccuser/shared/emily_files/australia_nigra_raw_reads/trimmed/rcorrector/SNF12L_S5_L001_2.cor.fq
...

```
The last two columns contain the **full** location to the desired reads.

#### Script
```{bash, eval = FALSE}
#!/bin/bash

#Create arguements
samples_file=$1 #File containing sample names and locations
out_dir_name=$2 #Desired name for the output directory

sudo docker run --rm -v`pwd`:`pwd` trinityrnaseq/trinityrnaseq Trinity --seqType fq --max_memory 188G \
        --samples_file $1 \
        --normalize_by_read_set \
        --CPU 16 \
        --output `pwd`/$2

```

Trinity was run inside of a **screen** session as `bash ../scripts/trinity_run.sh `pwd`/SNI_aus_samples.txt aus_nigra_trinity`. The `pwd` was added in front of the samples file name as a shortcut since Trinity needs to full path to the file (since it was installed as a docker image).

If you detach from the **screen** session, you can resume the session using `screen -r`. 

# Post-Assembly Quality Control
## Using BUSCO to assess composition or 'completeness' of assembly
[BUSCO](https://busco.ezlab.org/busco_userguide.html) (Benchmarking Universal Single-Copy Orthologs) maintains curated sets of universal single-copy genes from OrthoDB and assess the completeness if an assembly by how many of the **universal** genes have matches in the input data and whether those matches are duplicated, fragmented, or full length. 


### Running BUSCO on a single transcriptome
The lineage for _S. argus_ was chosen as `actinopterygii_odb10` since it is the closest clade provided with BUSCO.

```{bash, eval = FALSE}
#!/bin/bash

#Create arguments
transcriptome=$1 #Output fasta file from Trinity
lineage=$2 #chosen dataset for assessment
out_dir_name=$3 #Desired name for the output directory

busco -i $1 -l $2 -m tran -o $3 -c 16

```
This script was run as `nohup bash ../scripts/busco_run.sh aus_nigra_trinity.Trinity.fasta actinopterygii_odb10 aus_nigra_busco > busco.log 2>&1 &`

### Interpreting BUSCO results {.tabset}
The main results from BUSCO are simplified into categories of _Complete and single-copy_, _Complete and duplicated_, _Fragmented_, or _Missing_. The results from BUSCO will only make sense in the context of the biology of your organism and data types.

The transcriptome was not filtered for isoforms prior to running BUSCO which may then lead to a high proportion of duplicates. Additionally, not all BUSCO genes are necessarily expressed in the tissue/organ under consideration or at that specific timepoint. Therefore, transcriptomes are expected to show varying degrees of completeness. 

#### Short Summary {-}
```
# BUSCO version is: 5.2.2
# The lineage dataset is: actinopterygii_odb10 (Creation date: 2024-01-08, number of genomes: 26, number of BUSCOs: 3># Summarized benchmarking in BUSCO notation for file /home/rccuser/shared/emily_files/australia_nigra_raw_reads/aus_n># BUSCO was run in mode: transcriptome

        ***** Results: *****

        C:88.7%[S:18.9%,D:69.8%],F:2.2%,M:9.1%,n:3640
        3226    Complete BUSCOs (C)
        687     Complete and single-copy BUSCOs (S)
        2539    Complete and duplicated BUSCOs (D)
        80      Fragmented BUSCOs (F)
        334     Missing BUSCOs (M)
        3640    Total BUSCO groups searched

Dependencies and versions:
        hmmsearch: 3.1
        metaeuk: 6.a5d39d9
```

## Checking quality with tools installed in Trinity {.tabset}
Because of the way Trinity was installed on the RCC, all of the additional tools have to be accessed through the docker. The path for us is `/usr/local/bin/util/...`.

### Trinity transcriptome contig Nx Statistics {-}
The Nx statistic is calculated based on the lengths of the assembled transcriptome contigs. This statistic tells us that at least X% of assembled transcript nucleotides are found in contigs that are of Nx length.

  - Traditionally, you compute the N50 where at least **half** of all assembled bases are in transcript contigs of at LEAST the N50 length value.
  - This can be done using the built in TrinityStats.pl script within trinity.
  - The input for this function is the resulting _.Trinity.fasta_ file from the Trinity run.

```{bash, eval = FALSE}
#!/bin/bash

##Create the arguments
trinity_fasta_file=$1 #This is the output .fasta file from your Trinity run
trinity_stats_output=$2 #Desired name for your output results

sudo docker run -v`pwd`:`pwd` trinityrnaseq/trinityrnaseq /usr/local/bin/util/TrinityStats.pl \
        $trinity_fasta_file \
        >> $trinity_stats_output

```
  
This script was run as `bash ../scripts/trinity_N50.sh `pwd`/aus_nigra_trinity.Trinity.fasta trinity_stats_output`.

The contig N50 values are often exaggerated due to an assembly program generating too many transcript isoforms. To help mitigate this, we can look at the Nx values based on using only the single longest isoform per 'gene'. These Nx values are often lower than the Nx stats based on all assembled contigs.

#### Results {-}
```
################################
## Counts of transcripts, etc.
################################
Total trinity 'genes':  559044
Total trinity transcripts:      665636
Percent GC: 44.04

########################################
Stats based on ALL transcript contigs:
########################################

        Contig N10: 5216
        Contig N20: 3666
        Contig N30: 2674
        Contig N40: 1836
        Contig N50: 1010

        Median contig length: 277
        Average contig: 553.41
        Total assembled bases: 368368825


#####################################################
## Stats based on ONLY LONGEST ISOFORM per 'GENE':
#####################################################

        Contig N10: 3720
        Contig N20: 1784
        Contig N30: 611
        Contig N40: 409
        Contig N50: 338

        Median contig length: 264
        Average contig: 380.57
        Total assembled bases: 212752650
```

Here we can see that 50% of the assembled bases are found in transcript contigs that are at least 338 bases in length (going off the stats based on the longest isoform). This is significantly less than the NZ _nigra_, which had an N50 of 1104.

# Alignment and Abundance Estimations
## Using Salmon
### Installing Salmon
Salmon was originally installed in a conda environment, however, there were issues getting conda to install the latest version of salmon and so the docker image of it was used instead:
```
docker pull combinelab/salmon

#Check version
sudo docker run --rm -v`pwd`:`pwd` combinelab/salmon salmon --version
  salmon 1.10.3

```

### Generating the index
The function `index` was then used to generate an index on the transcriptome. This index is the structure that salmon uses to quasi-map RNA-seq reads during the quantification step. **This index has to be created only once per transcriptome**

```{bash, eval = FALSE}
#!/bin/bash

#Create arguments
transcriptome_file=$1
desired_index_name=$2
kmer_length=$3

/usr/local/bin/salmon index -t $1 -i $2 -k $3

```

The above script was run as: `nohup bash scripts/salmon_txome_indicies.sh S_argus_trinity.Trinity.fasta argus_salmon_index 31 > salmon_index.log 2>&1 &`.

A k-mer length of 31 was chosen as a recommendation from the [salmon documentation](https://salmon.readthedocs.io/en/latest/salmon.html#using-salmon). 

### Quantifying the samples
Once the index is built, the samples can be quantified.

```{bash, eval = FALSE}
#!/bin/bash

#Create arguments
input_dir=$1
index_file=$2
output_dir=$3

for fq in $1/*_1.cor.fq
        do

        #Extract sample name from the file
        sample=$(basename $fq _1.cor.fq)

        #Echo sample name that is currently running
        echo "Processing sample ${sample} ..."

        #Quantify this pair of reads
        /usr/local/bin/salmon quant -i $index_file -l A \
                -1 $1/${sample}_1.cor.fq \
                -2 $1/${sample}_2.cor.fq \
                -p 16 -o $3/${sample}_quant

done
```

This script was run as: `nohup bash ../scripts/salmon_quant.sh trimmed/rcorrector/ aus_nigra_salmon_index/ aus_nigra_salmon_quant > salmon_quant.log 2>&1 &`

Running the above script generates an output with the following organisation:
```
/home/rccuser/shared/emily_files/australia_nigra_raw_reads/
  aus_nigra_salmon_quant/
    SNM4AJT_S16_L001_quant/
      aux_info/  
      cmd_info.json  
      lib_format_counts.json  
      libParams/  
      logs/  
      quant.sf
```
The `quant.sf` file is what is going to be important for the next steps of calculating gene expression as it contains all of the quantification info generated for that sample. The `salmon_quant.log` is also useful and contains information about the mapping rate for each sample, which is a good metric for assembly quality. In general we want to see around 80% of the raw reads mapping back.

To make life easier, the quant.sf files and salmon_quant.log files were moved from their nested location to a shared folder and renamed to include the corresponding sample names like so:
```{bash, eval = FALSE}
#!/bin/bash

#Create arguements
input_dir=$1 #This is the location of all the Salmon quant folders that were generated
out_dir1=$2 #Desired location for the new renamed quant.sf files
out_dir2=$3 #Desired location for the new renamed salmon_quant.log files

for dir in $input_dir*_quant
    do

    echo "$dir"

    #Extract the sample name from the directory
    sample=$(basename $dir _quant)

    #Echo the sample that is being processed
    echo "Rename sample ${sample} ..."

    #Rename the quant.sf file
    mv $dir/quant.sf $out_dir1/${sample}_quant.sf

    #Rename the log file
    mv $dir/logs/salmon_quant.log $out_dir2/${sample}_salmon_quant.log

done

```
This script was run as `bash ../scripts/rename_salmon.sh aus_nigra_salmon_quant/ aus_nigra_salmon_quant/expression_files/ aus_nigra_salmon_quant/log_files/`.

The mapping rate was pulled from all of the log files with `grep "Mapping rate" *log > map.txt` and the results are below:
```
SNF11G_S1_L001_salmon_quant.log:[2025-03-03 09:52:16.455] [jointLog] [info] Mapping rate = 99.7415%
SNF11L_S2_L001_salmon_quant.log:[2025-03-03 09:59:26.209] [jointLog] [info] Mapping rate = 96.6081%
SNF11O_S3_L001_salmon_quant.log:[2025-03-03 10:02:54.283] [jointLog] [info] Mapping rate = 99.557%
SNF12G_S4_L001_salmon_quant.log:[2025-03-03 10:09:16.666] [jointLog] [info] Mapping rate = 99.8431%
SNF12L_S5_L001_salmon_quant.log:[2025-03-03 10:22:08.899] [jointLog] [info] Mapping rate = 66.3544%
SNF4A3G_S6_L001_salmon_quant.log:[2025-03-03 10:25:59.297] [jointLog] [info] Mapping rate = 99.881%
SNF4A3L_S7_L001_salmon_quant.log:[2025-03-03 10:39:14.610] [jointLog] [info] Mapping rate = 68.8287%
SNM29G_S8_L001_salmon_quant.log:[2025-03-03 10:41:32.561] [jointLog] [info] Mapping rate = 99.7039%
SNM29L_S9_L001_salmon_quant.log:[2025-03-03 10:49:15.230] [jointLog] [info] Mapping rate = 77.1082%
SNM29T_S10_L001_salmon_quant.log:[2025-03-03 10:52:54.793] [jointLog] [info] Mapping rate = 78.4749%
SNM3AJG_S11_L001_salmon_quant.log:[2025-03-03 10:55:38.473] [jointLog] [info] Mapping rate = 99.6117%
SNM3AJL_S12_L001_salmon_quant.log:[2025-03-03 11:07:42.257] [jointLog] [info] Mapping rate = 81.5231%
SNM3AJT_S13_L001_salmon_quant.log:[2025-03-03 11:10:22.983] [jointLog] [info] Mapping rate = 99.9147%
SNM4AJG_S14_L001_salmon_quant.log:[2025-03-03 11:21:18.805] [jointLog] [info] Mapping rate = 99.3639%
SNM4AJL_S15_L001_salmon_quant.log:[2025-03-03 11:28:20.766] [jointLog] [info] Mapping rate = 77.0104%
SNM4AJT_S16_L001_salmon_quant.log:[2025-03-03 11:30:06.239] [jointLog] [info] Mapping rate = 99.895%
```
# Assembly thinning and redundancy reduction

## Using Super transcripts
A SuperTranscript is constructed by collapsing unique and common sequence regions among your splicing isoforms into a single linear sequence. While the resulting SuperTranscript may not necessarily exist in a real biological context, they allow for assembly thinning to occur without any sequence loss. Here assembly thinning is not the main objective, but rather a nice side effect.

```{bash, eval = FALSE}
#!/bin/bash

trinity_out=$1 #The .fasta file generated by the Trinity run


sudo docker run -v`pwd`:`pwd` trinityrnaseq/trinityrnaseq /bin/sh -c "cd /home/rccuser/shared/emily_files/ && /usr/local/bin/Analysis/SuperTranscripts/Trinity_gene_splice_modeler.py --incl_malign --trinity_fasta $trinity_out"

```

This script was run as `nohup bash ../scripts/super_transcripts.sh `pwd`/aus_nigra_trinity.Trinity.fasta > super.log 2>&1 &`

## Re-evaluating quality of the assembly {.tabset}
### BUSCO {-}
#### Short Summary {-}
```