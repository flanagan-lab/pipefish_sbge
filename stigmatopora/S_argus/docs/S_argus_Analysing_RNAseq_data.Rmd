---
title: "Analysing *Stigmatopora argus* RNAseq Data"
author: "Emily Beasley"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: yes
    keep_tex: true
  html_document:
    code_folding: show
    toc: yes
    toc_float: yes
    number_sections: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir='../',fig_path="../imgs/")
```

``` {r library, include = FALSE}
#library(tximport) #This was used to generate an abundance matrix from the salmon quant outputs

```
This pipeline follows a pipeline outlined for _Syngnathus floridae_. Changes in scripts and outputs are outlined in this document. This pipeline was run on the University of Canterbury's RCC.

# Pre-assembly Quality Control and Filtering

## Trimming the Reads with Trimmomatic
[Trimmomatic](http://www.usadellab.org/cms/index.php?page=trimmomatic) is commonly used for Illumina paired-end and single ended data.  

### Running trimmomatic across Multiple Pairs of Reads

```{bash, eval = FALSE}
#!/bin/bash

data=$1

for fq in ${data}*_R1_001.fastq.gz
        do
        base=$(basename $fq _R1_001.fastq.gz)
        echo "Running trimmomatic for ${base}..."
        time trimmomatic PE -threads 16 $fq ${data}${base}_R2_001.fastq.gz \
                ${data}trimmed/${base}_paired_R1_001.fastq.gz ${data}trimmed/${base}_unpaired_R1_001.fastq.gz \
                ${data}trimmed/${base}_paired_R2_001.fastq.gz ${data}trimmed/${base}_unpaired_R2_001.fastq.gz \
                HEADCROP:12 LEADING:3 TRAILING:10 SLIDINGWINDOW:4:15 MINLEN:50
done

```



-   Before running the script, change the `trimmomatic` line to `echo "..."` to make sure all of the variables are working correctly.

-   Trimmomatic takes about a day to run, so either run it with nohup or screen 

-   There was no adapter contamination detected in these samples, so that option was not included as an option for trimming of _S. argus_ reads.  

-   Then remove the `echo "..."` and run the script as `nohup bash ../scripts/trim_no_adapter_run.sh ../S_argus_raw/ > trim.out 2>&1 &`.




## Using SortMeRNA to remove rRNA contamination

### Creating a for loop to run multiple samples at a time
```{bash, eval = FALSE}
#!/bin/bash

#Create arguments
input_dir=$1 #location of the reads
ref_fasta=$2 #desired reference fasta
output_dir_rrna=$3 #desired location for the reads that are rRNA
output_dir_norrna=$4 #desired location for the reads that are NOT rRNA

## Loop through all pairs of reads in the input directory
for pair in $1/*_R1_001.fastq.gz
        do

        #Extract the sample name from the file name
        sample=$(basename $pair _R1_001.fastq.gz)

        #Extract Fish ID
        ID=$(basename $pair _paired_R1_001.fastq.gz)

        ##Echo the sample name it is currently running
        echo "Running SortMeRNA for ${ID}..."

        #Define the paths to the input and output files for this sample
        input1=$1/${sample}_R1_001.fastq.gz
        input2=$1/${sample}_R2_001.fastq.gz

        #Run SortMeRNA on this pair of reads
        time sortmerna --threads 16 --ref $2 --reads $input1 --reads $input2 --fastx --aligned $3/${ID} --other $4/${ID} --out2

        #Remove SortMeRNA intermediate files before running again
        rm -r /home/rccuser/sortmerna/run/kvdb

done


```

This script was run as `nohup bash ../scripts/sortmerna_run.sh trimmed/ /home/rccuser/shared/rRNA_databases_v4.3.4/smr_v4.3_fast_db.fasta rrna/ no_rrna/ > sortmerna.log 2>&1 &`


## Using Kraken2 to remove biological contamination

### Creating a for loop to run multiple samples at a time
```{bash, eval = FALSE}

#!/bin/bash

#Create arguments
ref_fasta=$1 #desired reference database
input_dir=$2 #Input directory with the location of the reads
output_dir=$3 #Desired location of the output

## Loop through all pairs of reads in the input directory
for pair in $2/*_fwd.fq.gz
        do

        #Extract the sample name from the file name
        sample=$(basename $pair _fwd.fq.gz)

        ##Echo the sample name it is currently running
        echo "Running Kraken2 for ${sample}..."

        #Define the paths to the input and output files for this sample
        input1=$2/${sample}_fwd.fq.gz
        input2=$2/${sample}_rev.fq.gz

        #Run Kraken2 on this pair of reads
        time kraken2 --threads 16 --db $1 --paired $input1 $input2 --unclassified-out $3/${sample}#.fq --report $3/${sample}.log

done

```


This script was run as `nohup bash ../scripts/kraken2_run.sh /home/rccuser/shared/kraken2_pluspfp/ no_rrna/ argus_kraken2/ > kraken2.log 2>&1 &`

## Doing a k-mer based correction with RCorrector
Rcorrector (RNA-seq error CORRECTOR) is a kmer-based error correction method that is used on RNA-seq data. This program allows for a correction of random sequencing errors in Illumina RNA-seq reads. The downside is you may lose information on things like SNPs and other sequence variants.


### Creating a for loop to run multiple samples at a time
```{bash, eval = FALSE}
### Creating a for loop to run multiple samples at a time
#!/bin/bash

#Create arguments
rcorrector_path=$1 #Path to the run_rcorrector.pl file
input_dir=$2 #Path to the input directory containing the reads
output_dir=$3 #Path to the desired output location

##Loop through all pairs of reads in the directory
for pair in $2/*_1.fq
        do

        #Extract sample name from the file name
        sample=$(basename $pair _1.fq)

        #Echo the sample name that is currently running
        echo "Running rcorrector for ${sample} ..."

        #Run rcorrector on this pair of reads
        time perl $1/run_rcorrector.pl -t 16 -1 $2/${sample}_1.fq -2 $2/${sample}_2.fq -od $3

done

```

This script was run as `nohup bash ../scripts/rcorrector_run.sh /home/rccuser/rcorrector/ argus_kraken2/ rcorrector/ > rcor.log 2>&1 &`

# Checking quality of trimmed and filtered reads
The quality of the reads once they finished going through the filtering and trimming pipeline outlined above was assessed with FastQC and the results were compiled using MultiQC.

```{bash, eval = FALSE}
!/bin/bash

data=$1

for fq in ${data}*.gz
        do
        base=$(basename $fq)
        echo "Running fastqc for ${base} ..."
        time fastqc $fq -t 16 -o ${data}FastQC_trimmed
done

```

This script was run as `nohup bash scripts/fastqc_run.sh S_argus_raw/trimmed/paired/ > fastqc.log 2>&1 &`. Once finished MultiQC was run: `multiqc S_argus_FastQC`. 

# De novo transcriptome assembly

## Installing Trinity
Trinity requires the installation of several programs, to get around this Trinity was installed via a Docker in the RCC and must be used within one. **The first time that you use Trinity you must pull the latest Docker image for Trinity** like so:

```
docker pull trinityrnaseq/trinityrnaseq

```

## Running Trinity 
Once started, a successful Trinity run will take days to complete. There are a lot of extra components you can add onto a Trinity run (View the [Trinity User Manual](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running-Trinity) for more information).
    
### Using multiple reads
#### Creating a Sample Name File
There are two options when using multiple files to generate your de novo transcriptome assembly. Each sample can be typed out individually, ensuring that the two reads are paired up by order. Alternatively, the argument `--samples_file` can be used instead. With this argument you must provide a tab-delimited text file indicating biological replicate relationships. This is the example provided on the [Trinity Website](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Running-Trinity):

         cond_A    cond_A_rep1    A_rep1_left.fq    A_rep1_right.fq
         cond_A    cond_A_rep2    A_rep2_left.fq    A_rep2_right.fq
         cond_B    cond_B_rep1    B_rep1_left.fq    B_rep1_right.fq
         cond_B    cond_B_rep2    B_rep2_left.fq    B_rep2_right.fq
         
The samples file that was provided for the script below was adapted into this format:

```
T1F1O	S17	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T1F1O_S17_L001_1.cor.fq	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T1F1O_S17_L001_2.cor.fq
T1F3O	S18	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T1F3O_S18_L001_1.cor.fq	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T1F3O_S18_L001_2.cor.fq
T2F2G	S19	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T2F2G_S19_L001_1.cor.fq	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T2F2G_S19_L001_2.cor.fq
T2F2L	S20	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T2F2L_S20_L001_1.cor.fq	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T2F2L_S20_L001_2.cor.fq
T2F2O	S21	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T2F2O_S21_L001_1.cor.fq	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T2F2O_S21_L001_2.cor.fq
T6F1G	S22	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T6F1G_S22_L001_1.cor.fq	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T6F1G_S22_L001_2.cor.fq
T6F1L	S23	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T6F1L_S23_L001_1.cor.fq	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T6F1L_S23_L001_2.cor.fq
T6M5G	S24	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T6M5G_S24_L001_1.cor.fq	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T6M5G_S24_L001_2.cor.fq
T6M5L	S25	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T6M5L_S25_L001_1.cor.fq	/home/rccuser/shared/emily_files/S_argus_raw/rcorrector/T6M5L_S25_L001_2.cor.fq

...
```
The last two columns contain the **full** location to the desired reads.

#### Script
```{bash, eval = FALSE}
#!/bin/bash

#Create arguements
samples_file=$1 #File containing sample names and locations
out_dir_name=$2 #Desired name for the output directory

sudo docker run --rm -v`pwd`:`pwd` trinityrnaseq/trinityrnaseq Trinity --seqType fq --max_memory 188G \
        --samples_file $1 \
        --normalize_by_read_set \
        --CPU 16 \
        --output `pwd`/$2

```

Trinity was run inside of a **screen** session as `bash scripts/trinity_run.sh `pwd`/SA_samples.txt S_argus_trinity`. The `pwd` was added in front of the samples file name as a shortcut since Trinity needs to full path to the file (since it was installed as a docker image).

# Post-Assembly Quality Control
## Using BUSCO to assess composition or 'completeness' of assembly
[BUSCO](https://busco.ezlab.org/busco_userguide.html) (Benchmarking Universal Single-Copy Orthologs) maintains curated sets of universal single-copy genes from OrthoDB and assess the completeness if an assembly by how many of the **universal** genes have matches in the input data and whether those matches are duplicated, fragmented, or full length. 


### Running BUSCO on a single transcriptome
The lineage for _S. argus_ was chosen as `actinopterygii_odb10` since it is the closest clade provided with BUSCO.

```{bash, eval = FALSE}
#!/bin/bash

#Create arguments
transcriptome=$1 #Output fasta file from Trinity
lineage=$2 #chosen dataset for assessment
out_dir_name=$3 #Desired name for the output directory

busco -i $1 -l $2 -m tran -o $3 -c 16

```
This script was run as `nohup bash scripts/busco_run.sh S_argus_trinity.Trinity.fasta actinopterygii_odb10 busco_argus > busco.log 2>&1 &`

### Interpreting BUSCO results {.tabset}
The main results from BUSCO are simplified into categories of _Complete and single-copy_, _Complete and duplicated_, _Fragmented_, or _Missing_. The results from BUSCO will only make sense in the context of the biology of your organism and data types.

The transcriptome was not filtered for isoforms prior to running BUSCO which may then lead to a high proportion of duplicates. Additionally, not all BUSCO genes are necessarily expressed in the tissue/organ under consideration or at that specific timepoint. Therefore, transcriptomes are expected to show varying degrees of completeness. 

#### Short Summary {-}
```
# BUSCO version is: 5.2.2
# The lineage dataset is: actinopterygii_odb10 (Creation date: 2024-01-08, number of genomes: 26, number of BUSCOs: 3640)
# Summarized benchmarking in BUSCO notation for file /home/rccuser/shared/emily_files/S_argus_trinity.Trinity.fasta
# BUSCO was run in mode: transcriptome

        ***** Results: *****

        C:90.4%[S:13.8%,D:76.6%],F:2.3%,M:7.3%,n:3640
        3292    Complete BUSCOs (C)
        504     Complete and single-copy BUSCOs (S)
        2788    Complete and duplicated BUSCOs (D)
        83      Fragmented BUSCOs (F)
        265     Missing BUSCOs (M)
        3640    Total BUSCO groups searched

Dependencies and versions:
        hmmsearch: 3.1
        metaeuk: 6.a5d39d9
```


## Checking quality with tools installed in Trinity {.tabset}
Because of the way Trinity was installed on the RCC, all of the additional tools have to be accessed through the docker. The path for us is `/usr/local/bin/util/...`.

### Trinity transcriptome contig Nx Statistics {-}
The Nx statistic is calculated based on the lengths of the assembled transcriptome contigs. This statistic tells us that at least X% of assembled transcript nucleotides are found in contigs that are of Nx length.

  - Traditionally, you compute the N50 where at least **half** of all assembled bases are in transcript contigs of at LEAST the N50 length value.
  - This can be done using the built in TrinityStats.pl script within trinity.
  - The input for this function is the resulting _.Trinity.fasta_ file from the Trinity run.

```{bash, eval = FALSE}
#!/bin/bash

##Create the arguments
trinity_fasta_file=$1 #This is the output .fasta file from your Trinity run
trinity_stats_output=$2 #Desired name for your output results

sudo docker run -v`pwd`:`pwd` trinityrnaseq/trinityrnaseq /usr/local/bin/util/TrinityStats.pl \
        $trinity_fasta_file \
        >> $trinity_stats_output

```

This script was run as `bash scripts/trinity_N50.sh `pwd`/S_argus_trinity.Trinity.fasta trinity_stats_output`.

The contig N50 values are often exaggerated due to an assembly program generating too many transcript isoforms. To help mitigate this, we can look at the Nx values based on using only the single longest isoform per 'gene'. These Nx values are often lower than the Nx stats based on all assembled contigs.

#### Results {-}
```
################################
## Counts of transcripts, etc.
################################
Total trinity 'genes':  165990
Total trinity transcripts:      276261
Percent GC: 47.17

########################################
Stats based on ALL transcript contigs:
########################################

        Contig N10: 6003
        Contig N20: 4530
        Contig N30: 3563
        Contig N40: 2857
        Contig N50: 2246

        Median contig length: 418
        Average contig: 1020.08
        Total assembled bases: 281807618


#####################################################
## Stats based on ONLY LONGEST ISOFORM per 'GENE':
#####################################################

        Contig N10: 5598
        Contig N20: 3940
        Contig N30: 2861
        Contig N40: 1988
        Contig N50: 1172

        Median contig length: 316
        Average contig: 646.83
        Total assembled bases: 107367912
```
Here we can see that 50% of the assembled bases are found in transcript contigs that are at least 1172 bases in length (going off the stats based on the longest isoform).



# Alignment and Abundance Estimations


### Generating the index
The function `index` was then used to generate an index on the transcriptome. This index is the structure that salmon uses to quasi-map RNA-seq reads during the quantification step. **This index has to be created only once per transcriptome**

```{bash, eval = FALSE}
#!/bin/bash

#Create arguments
transcriptome_file=$1
desired_index_name=$2
kmer_length=$3

/usr/local/bin/salmon index -t $1 -i $2 -k $3

```

The above script was run as: `nohup bash scripts/salmon_txome_indicies.sh S_argus_trinity.Trinity.fasta argus_salmon_index 31 > salmon_index.log 2>&1 &`.

A k-mer length of 31 was chosen as a recommendation from the [salmon documentation](https://salmon.readthedocs.io/en/latest/salmon.html#using-salmon). 

### Quantifying the samples
Once the index is built, the samples can be quantified.

```{bash, eval = FALSE}
#!/bin/bash

#Create arguments
input_dir=$1
index_file=$2
output_dir=$3

for fq in $1/*_1.cor.fq
        do

        #Extract sample name from the file
        sample=$(basename $fq _1.cor.fq)

        #Echo sample name that is currently running
        echo "Processing sample ${sample} ..."

        #Quantify this pair of reads
        /usr/local/bin/salmon quant -i $index_file -l A \
                -1 $1/${sample}_1.cor.fq \
                -2 $1/${sample}_2.cor.fq \
                -p 16 -o $3/${sample}_quant

done
```

This script was run as: `nohup bash scripts/salmon_quant.sh S_argus_2024/rcorrector argus_salmon_index/ argus_salmon_quant/ > salmon_quant.log 2>&1 &`

Running the above script generates an output with the following organization:
```
argus_salmon_quant/
    T1F1O_S17_L001_quant/
      aux_info/
      cmd_info.json
      lib_format_counts.json
      libParams/
      logs/
        salmon_quant.log
      quant.sf
      
    ... for all samples
```
The `quant.sf` file is what is going to be important for the next steps of calculating gene expression as it contains all of the quantification info generated for that sample. The `salmon_quant.log` is also useful and contains information about the mapping rate for each sample, which is a good metric for assembly quality. In general we want to see around 80% of the raw reads mapping back.

To make life easier, the quant.sf files and salmon_quant.log files were moved from their nested location to a shared folder and renamed to include the corresponding sample names like so:
```{bash, eval = FALSE}
#!/bin/bash

#Create arguements
input_dir=$1 #This is the location of all the Salmon quant folders that were generated
out_dir1=$2 #Desired location for the new renamed quant.sf files
out_dir2=$3 #Desired location for the new renamed salmon_quant.log files

for dir in $input_dir*_quant
    do

    echo "$dir"

    #Extract the sample name from the directory
    sample=$(basename $dir _quant)

    #Echo the sample that is being processed
    echo "Rename sample ${sample} ..."

    #Rename the quant.sf file
    mv $dir/quant.sf $out_dir1/${sample}_quant.sf

    #Rename the log file
    mv $dir/logs/salmon_quant.log $out_dir2/${sample}_salmon_quant.log

done

```
This script was run as `bash scripts/rename_salmon.sh argus_salmon_quant/ argus_salmon_quant/expression_files/ argus_salmon_quant/log_files/`.

The mapping rate was pulled from all of the log files with `grep "Mapping rate" *log > map.txt` and the results are below:
```
T1F1O_S17_L001_salmon_quant.log:[2024-02-01 15:06:22.939] [jointLog] [info] Mapping rate = 97.6202%
T1F3O_S18_L001_salmon_quant.log:[2024-02-01 15:07:16.194] [jointLog] [info] Mapping rate = 77.5963%
T2F2G_S19_L001_salmon_quant.log:[2024-02-01 15:08:53.120] [jointLog] [info] Mapping rate = 99.7665%
T2F2L_S20_L001_salmon_quant.log:[2024-02-01 15:14:00.417] [jointLog] [info] Mapping rate = 99.837%
T2F2O_S21_L001_salmon_quant.log:[2024-02-01 15:17:10.993] [jointLog] [info] Mapping rate = 77.6776%
T6F1G_S22_L001_salmon_quant.log:[2024-02-01 15:18:53.280] [jointLog] [info] Mapping rate = 94.3362%
T6F1L_S23_L001_salmon_quant.log:[2024-02-01 15:19:47.790] [jointLog] [info] Mapping rate = 99.8096%
T6M5G_S24_L001_salmon_quant.log:[2024-02-01 15:22:17.229] [jointLog] [info] Mapping rate = 99.6736%
T6M5L_S25_L001_salmon_quant.log:[2024-02-01 15:25:28.804] [jointLog] [info] Mapping rate = 97.0366%
T6M5T_S26_L001_salmon_quant.log:[2024-02-01 15:26:50.378] [jointLog] [info] Mapping rate = 76.1502%
T7F3G_S27_L001_salmon_quant.log:[2024-02-01 15:29:08.527] [jointLog] [info] Mapping rate = 99.8446%
T7F3L_S28_L001_salmon_quant.log:[2024-02-01 15:34:37.165] [jointLog] [info] Mapping rate = 99.7689%
T7F3O_S29_L001_salmon_quant.log:[2024-02-01 15:39:16.545] [jointLog] [info] Mapping rate = 78.6607%
T7F4G_S30_L001_salmon_quant.log:[2024-02-01 15:40:22.847] [jointLog] [info] Mapping rate = 97.5494%
T7F4L_S31_L001_salmon_quant.log:[2024-02-01 15:40:53.801] [jointLog] [info] Mapping rate = 99.8095%
T7F4O_S32_L001_salmon_quant.log:[2024-02-01 15:44:05.417] [jointLog] [info] Mapping rate = 77.4103%
T7M4G_S33_L001_salmon_quant.log:[2024-02-01 15:45:38.964] [jointLog] [info] Mapping rate = 99.6163%
T7M4L_S34_L001_salmon_quant.log:[2024-02-01 15:47:46.324] [jointLog] [info] Mapping rate = 99.7261%
T7M4T_S35_L001_salmon_quant.log:[2024-02-01 15:50:41.068] [jointLog] [info] Mapping rate = 85.7414%
T7M5G_S36_L001_salmon_quant.log:[2024-02-01 15:51:18.115] [jointLog] [info] Mapping rate = 99.6449%
T7M5L_S37_L001_salmon_quant.log:[2024-02-01 15:53:20.787] [jointLog] [info] Mapping rate = 99.7544%
T7M5T_S38_L001_salmon_quant.log:[2024-02-01 15:54:57.709] [jointLog] [info] Mapping rate = 80.5494%
T8F3G_S39_L001_salmon_quant.log:[2024-02-01 15:55:36.221] [jointLog] [info] Mapping rate = 63.1909%
T8F3L_S40_L001_salmon_quant.log:[2024-02-01 15:56:09.607] [jointLog] [info] Mapping rate = 77.284%
T8M3G_S41_L001_salmon_quant.log:[2024-02-01 15:57:55.529] [jointLog] [info] Mapping rate = 98.7314%
T8M3L_S42_L001_salmon_quant.log:[2024-02-01 15:59:35.411] [jointLog] [info] Mapping rate = 98.0724%
T8M3T_S43_L001_salmon_quant.log:[2024-02-01 16:00:30.825] [jointLog] [info] Mapping rate = 99.7675%
T8M4G_S44_L001_salmon_quant.log:[2024-02-01 16:01:51.514] [jointLog] [info] Mapping rate = 99.8104%
T8M4L_S45_L001_salmon_quant.log:[2024-02-01 16:09:24.634] [jointLog] [info] Mapping rate = 79.6761%
T8M4T_S46_L001_salmon_quant.log:[2024-02-01 16:10:55.086] [jointLog] [info] Mapping rate = 99.8069%
```


# Assembly thinning and redundancy reduction

## Using Super transcripts
A SuperTranscript is constructed by collapsing unique and common sequence regions among your splicing isoforms into a single linear sequence. While the resulting SuperTranscript may not necessarily exist in a real biological context, they allow for assembly thinning to occur without any sequence loss. Here assembly thinning is not the main objective, but rather a nice side effect.

```{bash, eval = FALSE}
#!/bin/bash

trinity_out=$1 #The .fasta file generated by the Trinity run


sudo docker run -v`pwd`:`pwd` trinityrnaseq/trinityrnaseq /bin/sh -c "cd /home/rccuser/shared/emily_files/ && /usr/local/bin/Analysis/SuperTranscripts/Trinity_gene_splice_modeler.py --incl_malign --trinity_fasta $trinity_out"

```

This script was run as `nohup bash scripts/super_transcripts.sh S_argus_trinity.Trinity.fasta > super.log 2>&1 &`

## Re-evaluating quality of the assembly {.tabset}
### BUSCO {-}
#### Short Summary {-}
```
# BUSCO version is: 5.2.2
# The lineage dataset is: actinopterygii_odb10 (Creation date: 2024-01-08, number of genomes: 26, number of BUSCOs: 3640)
# Summarized benchmarking in BUSCO notation for file /home/rccuser/shared/emily_files/super_trinity_genes.fasta
# BUSCO was run in mode: transcriptome

        ***** Results: *****

        C:86.7%[S:84.9%,D:1.8%],F:4.0%,M:9.3%,n:3640
        3157    Complete BUSCOs (C)
        3091    Complete and single-copy BUSCOs (S)
        66      Complete and duplicated BUSCOs (D)
        147     Fragmented BUSCOs (F)
        336     Missing BUSCOs (M)
        3640    Total BUSCO groups searched

Dependencies and versions:
        hmmsearch: 3.1
        metaeuk: 6.a5d39d9

```
## Re-Checking quality with tools installed in Trinity {.tabset}
Because of the way Trinity was installed on the RCC, all of the additional tools have to be accessed through the docker. The path for us is `/usr/local/bin/util/...`.

### Trinity transcriptome contig Nx Statistics {-}

```{bash, eval = FALSE}
#!/bin/bash

##Create the arguments
trinity_fasta_file=$1 #This is the output .fasta file from your Trinity run
trinity_stats_output=$2 #Desired name for your output results

sudo docker run -v`pwd`:`pwd` trinityrnaseq/trinityrnaseq /usr/local/bin/util/TrinityStats.pl \
        $trinity_fasta_file \
        >> $trinity_stats_output

```

This script was run as `bash scripts/trinity_N50.sh `pwd`/super_trinity_genes.fasta  super_stats_output`.

The contig N50 values are often exaggerated due to an assembly program generating too many transcript isoforms. To help mitigate this, we can look at the Nx values based on using only the single longest isoform per 'gene'. These Nx values are often lower than the Nx stats based on all assembled contigs.

#### Results {-}
```
################################
## Counts of transcripts, etc.
################################
Total trinity 'genes':  165990
Total trinity transcripts:      165990
Percent GC: 45.94

########################################
Stats based on ALL transcript contigs:
########################################

        Contig N10: 6603
        Contig N20: 4742
        Contig N30: 3498
        Contig N40: 2499
        Contig N50: 1546

        Median contig length: 321
        Average contig: 709.88
        Total assembled bases: 117833615


#####################################################
## Stats based on ONLY LONGEST ISOFORM per 'GENE':
#####################################################

        Contig N10: 6603
        Contig N20: 4742
        Contig N30: 3498
        Contig N40: 2499
        Contig N50: 1546

        Median contig length: 321
        Average contig: 709.88
        Total assembled bases: 117833615
```
Here we can see that 50% of the assembled bases are found in transcript contigs that are at least 1546 bases in length (going off the stats based on the longest isoform).



# Re-evaluating Alignment and Abundance Estimations
### Generating the index
The function `index` was then used to generate an index on the supertranscriptome. This index is the structure that salmon uses to quasi-map RNA-seq reads during the quantification step. 

```{bash, eval = FALSE}
#!/bin/bash

#Create arguments
transcriptome_file=$1
desired_index_name=$2
kmer_length=$3

/usr/local/bin/salmon index -t $1 -i $2 -k $3

```

The above script was run as: `nohup bash scripts/salmon_txome_indices.sh super_trinity_genes.fasta super_salmon_index 31 > super_index.log 2>&1 &`.

A k-mer length of 31 was chosen as a recommendation from the [salmon documentation](https://salmon.readthedocs.io/en/latest/salmon.html#using-salmon). 

### Re-Quantifying the samples
Once the index is built, the samples can be quantified.

```{bash, eval = FALSE}
#!/bin/bash

#Create arguments
input_dir=$1
index_file=$2
output_dir=$3

for fq in $1/*_1.cor.fq
        do

        #Extract sample name from the file
        sample=$(basename $fq _1.cor.fq)

        #Echo sample name that is currently running
        echo "Processing sample ${sample} ..."

        #Quantify this pair of reads
        /usr/local/bin/salmon quant -i $index_file -l A \
                -1 $1/${sample}_1.cor.fq \
                -2 $1/${sample}_2.cor.fq \
                -p 16 -o $3/${sample}_quant

done
```

This script was run as: `nohup bash scripts/salmon_quant.sh S_argus_2024/rcorrector/ super_salmon_index/ super_salmon_quant/ > super_quant.log 2>&1 &`

Running the above script generates an output with the following organization:
```
super_salmon_quant/
    T1F1O_S17_L001_quant/
      aux_info/
      cmd_info.json
      lib_format_counts.json
      libParams/
      logs/
        salmon_quant.log
      quant.sf
      
    ... for all samples
```
The `quant.sf` file is what is going to be important for the next steps of calculating gene expression as it contains all of the quantification info generated for that sample. The `salmon_quant.log` is also useful and contains information about the mapping rate for each sample, which is a good metric for assembly quality. In general we want to see around 80% of the raw reads mapping back.

To make life easier, the quant.sf files and salmon_quant.log files were moved from their nested location to a shared folder and renamed to include the corresponding sample names like so:
```{bash, eval = FALSE}
#!/bin/bash

#Create arguements
input_dir=$1 #This is the location of all the Salmon quant folders that were generated
out_dir1=$2 #Desired location for the new renamed quant.sf files
out_dir2=$3 #Desired location for the new renamed salmon_quant.log files

for dir in $input_dir*_quant
    do

    echo "$dir"

    #Extract the sample name from the directory
    sample=$(basename $dir _quant)

    #Echo the sample that is being processed
    echo "Rename sample ${sample} ..."

    #Rename the quant.sf file
    mv $dir/quant.sf $out_dir1/${sample}_quant.sf

    #Rename the log file
    mv $dir/logs/salmon_quant.log $out_dir2/${sample}_salmon_quant.log

done

```
This script was run as `bash scripts/rename_salmon.sh super_salmon_quant/ super_salmon_quant/expression_files/ super_salmon_quant/log_files/`.

The mapping rate was pulled from all of the log files with `grep "Mapping rate" *log > map.txt` and the results are below:
```
T1F1O_S17_L001_salmon_quant.log:[2024-02-02 12:37:13.596] [jointLog] [info] Mapping rate = 94.6708%
T1F3O_S18_L001_salmon_quant.log:[2024-02-02 12:37:44.584] [jointLog] [info] Mapping rate = 73.0562%
T2F2G_S19_L001_salmon_quant.log:[2024-02-02 12:38:20.585] [jointLog] [info] Mapping rate = 98.3122%
T2F2L_S20_L001_salmon_quant.log:[2024-02-02 12:38:53.995] [jointLog] [info] Mapping rate = 98.4674%
T2F2O_S21_L001_salmon_quant.log:[2024-02-02 12:39:31.510] [jointLog] [info] Mapping rate = 73.1606%
T6F1G_S22_L001_salmon_quant.log:[2024-02-02 12:39:40.640] [jointLog] [info] Mapping rate = 90.4676%
T6F1L_S23_L001_salmon_quant.log:[2024-02-02 12:40:13.619] [jointLog] [info] Mapping rate = 96.7354%
T6M5G_S24_L001_salmon_quant.log:[2024-02-02 12:40:42.176] [jointLog] [info] Mapping rate = 98.1889%
T6M5L_S25_L001_salmon_quant.log:[2024-02-02 12:40:54.542] [jointLog] [info] Mapping rate = 94.5837%
T6M5T_S26_L001_salmon_quant.log:[2024-02-02 12:41:35.889] [jointLog] [info] Mapping rate = 71.6815%
T7F3G_S27_L001_salmon_quant.log:[2024-02-02 12:42:10.220] [jointLog] [info] Mapping rate = 98.5669%
T7F3L_S28_L001_salmon_quant.log:[2024-02-02 12:42:47.084] [jointLog] [info] Mapping rate = 98.0709%
T7F3O_S29_L001_salmon_quant.log:[2024-02-02 12:43:32.402] [jointLog] [info] Mapping rate = 72.796%
T7F4G_S30_L001_salmon_quant.log:[2024-02-02 12:43:37.392] [jointLog] [info] Mapping rate = 94.8215%
T7F4L_S31_L001_salmon_quant.log:[2024-02-02 12:43:59.068] [jointLog] [info] Mapping rate = 98.3105%
T7F4O_S32_L001_salmon_quant.log:[2024-02-02 12:44:29.230] [jointLog] [info] Mapping rate = 93.7166%
T7M4G_S33_L001_salmon_quant.log:[2024-02-02 12:44:49.443] [jointLog] [info] Mapping rate = 98.2906%
T7M4L_S34_L001_salmon_quant.log:[2024-02-02 12:45:13.856] [jointLog] [info] Mapping rate = 97.8713%
T7M4T_S35_L001_salmon_quant.log:[2024-02-02 12:45:26.486] [jointLog] [info] Mapping rate = 95.6182%
T7M5G_S36_L001_salmon_quant.log:[2024-02-02 12:45:41.861] [jointLog] [info] Mapping rate = 98.018%
T7M5L_S37_L001_salmon_quant.log:[2024-02-02 12:45:56.010] [jointLog] [info] Mapping rate = 98.0401%
T7M5T_S38_L001_salmon_quant.log:[2024-02-02 12:46:23.222] [jointLog] [info] Mapping rate = 76.5435%
T8F3G_S39_L001_salmon_quant.log:[2024-02-02 12:46:29.371] [jointLog] [info] Mapping rate = 60.2095%
T8F3L_S40_L001_salmon_quant.log:[2024-02-02 12:46:52.401] [jointLog] [info] Mapping rate = 97.0916%
T8M3G_S41_L001_salmon_quant.log:[2024-02-02 12:47:09.242] [jointLog] [info] Mapping rate = 96.4127%
T8M3L_S42_L001_salmon_quant.log:[2024-02-02 12:47:23.864] [jointLog] [info] Mapping rate = 95.9183%
T8M3T_S43_L001_salmon_quant.log:[2024-02-02 12:47:35.286] [jointLog] [info] Mapping rate = 98.4497%
T8M4G_S44_L001_salmon_quant.log:[2024-02-02 12:48:18.428] [jointLog] [info] Mapping rate = 98.6341%
T8M4L_S45_L001_salmon_quant.log:[2024-02-02 12:48:39.000] [jointLog] [info] Mapping rate = 96.8233%
T8M4T_S46_L001_salmon_quant.log:[2024-02-02 12:49:23.744] [jointLog] [info] Mapping rate = 98.634%

```


Generating the transcript abundance was done on the RCC via R command line:
```{r tximport, eval=FALSE}
#Enter R
conda activate R
R

#Install and load required package
if (!require("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("tximport")
library(tximport)

#Read in sample file
samples <- read.table("SA_sample_ID.txt", header=TRUE)

#Create list of quant.sf files from salmon
files <- list.files("/home/rccuser/shared/emily_files/salmon_all/super_salmon_quant/expression_files", full.names = TRUE)
names(files) <- paste0(samples$ID)
all(file.exists(files))
  ## [1] TRUE
  
#Pull the transcript-gene relationship from the .gtf file generated during the SuperTranscripts step
gtf <- read.table("trinity_genes.gtf", header = FALSE)
tx2gene <- gtf[,c(10, 10)] #Using the geneID for both the gene and transcript ID as we no longer has isoform information in the SuperTranscripts assembly
tx2gene <- unique(tx2gene)
colnames(tx2gene) <- c("gene_id", "transcript_id")

#Building the matrix
txi.salmon.SA <- tximport(files, type = "salmon", tx2gene = tx2gene)
head(txi.salmon.SA$counts)
saveRDS(txi.salmon.SA, "txi.salmon_SA.RDS") #Move the file off the RCC to continue the Differential Expression analysis in R

```