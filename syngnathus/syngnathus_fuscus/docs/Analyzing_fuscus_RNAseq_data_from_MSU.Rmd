---
title: "Analyzing *Syngnathus fuscus* RNAseq Data from MSU"
author: "Coley Tosto"
date: "`r Sys.Date()`"
output:
  github_document:
    toc: true
    #html_document:
        #code_folding: show
        #toc: yes
        #toc_float: yes
        #number_sections: yes
---
```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir='../',fig_path="../figs/")
```

``` {r library, include = FALSE}
#library(tximport) #used to generate transcript abundance from salmon quant files
```

# Pre-assembly Quality Control and Filtering
These reads were obtained from the **RTSF Genomics Core** at Michigan State University. They were downloaded with the File Transfer Protocol provided on their [website.](https://rtsf.natsci.msu.edu/genomics/data-retrieval/)

Quality scores were provided by MSU, so FastQC was not run on the raw reads. The provided scores were used to assist in choosing the settings for trimming and filtering. The trimming and filtering for these reads will follow the pipeline that was laid out for _Syngnathus floridae_. Any changes to scripts or steps will be highlighted in this document. Analysis was conducted in a Remote Computing Cluster (RCC) at the University of Canterbury.

For _Syngnathus fuscus_ we started with an average of 50 &pm; 6.9 million reads per sample for a total of 2.99 billion reads.

## Trimming the raw reads with Trimmomatic

Trimmomatic was install via a conda environment `Trim` on the RCC. `trimmomatic v0.39` was used for the following script.

```{bash run_trimmomatic, file='bash/trim_script.sh', eval=FALSE}

```

-   Before running the script, change the `trimmomatic` line to `echo "..."` to make sure all of the variables are working correctly.

-   Then remove the `echo ""` and run the script as `nohup bash trim_script.sh > trim.out 2>&1 &`.\

-   The **NexteraPE-PE.fa** file was pulled from the [trimmomatic github](https://github.com/usadellab/Trimmomatic/tree/main/adapters) using `wget https://github.com/usadellab/Trimmomatic/blob/main/adapters/NexteraPE-PE.fa`

## Using Kraken2 to remove biological contamination

Kraken2 was installed in the conda environment `kraken2` on the RCC. `kraken2 v2.1.2` was used.

```{bash run_kraken2, file='bash/kraken2.sh', eval = FALSE}

```

This script was run as `nohup bash bash_scripts/kraken2.sh ../kraken2_pluspfp/ fuscus_trimmed fuscus_nobio > kraken2.log 2>&1 &`

The kraken2 database used for this analysis included the standard database (archea, bacteria, viral) plus plant, fungi, and protozoan datbases. Only reads that did not map back to these databases were retained.

## Using SortMeRNA to remove rRNA contamination

SortmeRNA was installed via a conda environment `sortmerna` on the RCC.`sortmerna v4.3.6` was used

```{bash run_sortmeRNA, file='bash/sortmeRNA.sh', eval = FALSE}

```

This script was run as `nohup bash bash_scripts/sortmerna.sh fuscus_trimmed ../rRNA_databases_v4.3.4/smr_v4.3_fast_db.fasta fuscus_rrna fuscus_norrna > sortmerna.log 2>&1 &`.

The chosed reference FASTA file contains **a subset of sequences** from the default SortMeRNA database. Here, the number of sequences in each database is reduced to improve the speed of the analysis.

## Doing a k-mer based correction with RCorrector

The Rcorrector github repo was cloned and Rcorrector was installed in the /shared folder on the RCC.

```{bash run_Rcorrector, file='bash/rcor.sh', eval = FALSE}

```

This script was run as `nohup bash bash_scripts/rcor.sh ../../rcorrector fuscus_nobio fuscus_kmer_corrected/ > rcor.log 2>&1 &`. After this step the fasta files were g-zipped.

# Checking quality of trimmed and filtered reads
The quality of the reads once they finished going through the filtering and trimming pipeline outlined above was assessed with FastQC and the results were compiled using MultiQC.

```{bash run_fastqc, file='bash/fastqc_script.sh', eval=FALSE}

```

This script was run as `nohup bash bash_scripts/fastqc_script.sh fuscus_kmer_corrected fuscus_FastQC > fastqc.log 2>&1 &`. Once finished MultiQC was run: `multiqc fuscus_FastQC`.

From the General stats reported in the MultiQC report we can see that following this filtering/trimming process we end up with an average of 42 &pm; 6.2 million reads per sample for a total of 2.54 billion reads.

# Mapping reads to _S. scovelli_ genome
There is currently a genome that exists for a closely related species within the same genus as _S. fuscus_. As a result of this, rather than constructing a _de novo_ transcriptome assembly, I am going to use that genome to map the reads back to.

There are several programs that have the capability of doing this. The two main things that I will be looking to increase are run time (how much space and time will running these programs take up) and accuracy (what proportion of my reads are mapping back to the genome). Some of the alignment programs include:

  - `STAR`
  - `HISAT2`
  - `Bowtie2`
  - `BWA`

A previous study comparing these different alignment software ([Musich et al., 2021](https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2021.657240/full)) highlighted that `Bowtie2` with local alignment mode and `BWA` had the highest alignment rate followed by `STAR` and lastly `HISAT2`. However, `Bowtie2` with the local alignment took the longest to run followed by `STAR` and `BWA` which were similar and lastly, `HISAT2` which was by far the fastest. For `HISAT2` and `STAR` it appears that they are able to better handle reads that are >1,000bp compared to the other aligners.

Outside of these components, one other important component to keep in mind is the fact that one data source is genomic while the other is transcriptomic. Where this presents potential issues is in the fragmentation associated with transcriptomic data, which when unrecognized, translates to large gaps in comparison to the genome. To mediate this, some of the aligners have been designed to recognize this, including `STAR` and `HISAT2`. They are able to do so by allowing the option of inputting a transcriptome alongside the reference.

Based on the above, I am going to use `STAR` as a starting point.

There are two major components involved with using `STAR`, generating a genome index and aligning the reads. The STAR commands on command line have the following format:

```
STAR --option1-name option1-value(s) --option2-name option2-value(s) . . .
```

## Indexing the genome an aligning reads with STAR
STAR was installed via a conda environment on the RCC called `STAR`. `STAR v 2.7.10b` was used.




# Alignment and abundance estimations {.tabset}
Salmon was installed locally on the RCC. It must be run as `/usr/local/bin/salmon`. `Salmon v 1.10.1` was used.

## Generate the index {-}
`salmon index` was used for this step. The index is built only **once** per transcriptome.

```{bash salmon-build-index, file='bash/salmon_txome_indicies.sh', eval = FALSE}

```

The script was run as `nohup bash bash_scripts/salmon_txome_indicies.sh trinity_out_dir_fuscus_June2023.Trinity.fasta fuscus_salmon_index 31 > salmon_index.log 2>&1 &`. A kmer length of 31 was used upon recommendation from the Salmon website.

## Quantify the samples {-}
After building the index this is run on every individual read pair.

```{bash salmon-quant, file='bash/salmon_quant.sh', eval=FALSE}

```

This script was run as `nohup bash bash_scripts/salmon_quant.sh fuscus_kmer_corrected/ fuscus_salmon_index/ fuscus_salmon_quant/ > salmon_quant.log 2>&1 &`.

The `quant.sf` and `.log` files were moved out of their nested location to a shared folder and renamed to include the sample name.
```{bash rename-salmon-files, file='bash/rename_salmon.sh', eval = FALSE}

```

The mapping rate was pulled from all of the log files with `grep "Mapping rate" *log > FUmap.txt` and the results are below. Generally, we want to see a mapping rate of at least 80% for our samples.
```
FUG10M2_salmon_quant.log:[2023-10-09 23:13:27.146] [jointLog] [info] Mapping rate = 99.8809%
FUG11F1_salmon_quant.log:[2023-10-09 23:43:18.088] [jointLog] [info] Mapping rate = 99.8791%
FUG11M2_salmon_quant.log:[2023-10-10 00:14:21.567] [jointLog] [info] Mapping rate = 99.8895%
FUG11M4_salmon_quant.log:[2023-10-10 00:45:11.435] [jointLog] [info] Mapping rate = 99.5617%
FUG12M1_salmon_quant.log:[2023-10-10 01:04:04.579] [jointLog] [info] Mapping rate = 99.8942%
FUG13F1_salmon_quant.log:[2023-10-10 01:43:52.069] [jointLog] [info] Mapping rate = 99.8912%
FUG13F4_salmon_quant.log:[2023-10-10 02:18:06.249] [jointLog] [info] Mapping rate = 99.9204%
FUG15M5_salmon_quant.log:[2023-10-10 02:51:42.322] [jointLog] [info] Mapping rate = 99.93%
FUG2F2_salmon_quant.log:[2023-10-10 03:17:58.246] [jointLog] [info] Mapping rate = 99.8933%
FUG3F2_salmon_quant.log:[2023-10-10 03:41:01.692] [jointLog] [info] Mapping rate = 99.8957%
FUL10M2_salmon_quant.log:[2023-10-10 04:12:32.747] [jointLog] [info] Mapping rate = 99.9229%
FUL11F1_salmon_quant.log:[2023-10-10 04:28:38.157] [jointLog] [info] Mapping rate = 99.935%
FUL11M2_salmon_quant.log:[2023-10-10 04:47:41.021] [jointLog] [info] Mapping rate = 99.9017%
FUL11M4_salmon_quant.log:[2023-10-10 05:02:14.331] [jointLog] [info] Mapping rate = 99.9295%
FUL12M1_salmon_quant.log:[2023-10-10 05:18:34.970] [jointLog] [info] Mapping rate = 99.9099%
FUL13F1_salmon_quant.log:[2023-10-10 05:39:55.564] [jointLog] [info] Mapping rate = 99.9373%
FUL13F4_salmon_quant.log:[2023-10-10 05:58:09.029] [jointLog] [info] Mapping rate = 99.9419%
FUL15M5_salmon_quant.log:[2023-10-10 06:14:57.386] [jointLog] [info] Mapping rate = 99.9355%
FUL2F2_salmon_quant.log:[2023-10-10 06:31:12.803] [jointLog] [info] Mapping rate = 78.6675%
FUL3F2_salmon_quant.log:[2023-10-10 06:40:34.545] [jointLog] [info] Mapping rate = 99.94%
FUO11F1_salmon_quant.log:[2023-10-10 07:02:53.060] [jointLog] [info] Mapping rate = 99.8901%
FUO13F1_salmon_quant.log:[2023-10-10 07:38:11.441] [jointLog] [info] Mapping rate = 99.905%
FUO13F4_salmon_quant.log:[2023-10-10 08:11:16.544] [jointLog] [info] Mapping rate = 99.9083%
FUO2F2_salmon_quant.log:[2023-10-10 08:58:16.701] [jointLog] [info] Mapping rate = 99.9399%
FUO3F2_salmon_quant.log:[2023-10-10 09:48:39.310] [jointLog] [info] Mapping rate = 99.832%
FUT10M2_salmon_quant.log:[2023-10-10 10:28:14.802] [jointLog] [info] Mapping rate = 99.9139%
FUT11M2_salmon_quant.log:[2023-10-10 11:10:17.014] [jointLog] [info] Mapping rate = 99.9051%
FUT11M4_salmon_quant.log:[2023-10-10 11:44:42.531] [jointLog] [info] Mapping rate = 99.8352%
FUT12M1_salmon_quant.log:[2023-10-10 12:10:03.402] [jointLog] [info] Mapping rate = 99.872%
FUT15M5_salmon_quant.log:[2023-10-10 12:38:18.215] [jointLog] [info] Mapping rate = 99.8842%
```
A very high mapping rate is seen for all but one sample, **FUL2F2**. If that stays the same after the assembly thinning I will go back and investigate why that may be. Overall, very happy with these results.

# Assembly thinning and redundancy reduction - SuperTranscripts
SuperTranscripts were generated to reduce redundancy while still retaining all of the sequence information. These were generated with the tool installed with Trinity.

```{bash super-transcript, file='bash/trin_super_transcripts.sh', eval=FALSE}

```

This script was run in a screen session as `bash bash_scripts/trin_super_transcripts.sh trinity_out_dir_fuscus_June2023.Trinity.fasta`. It took about 11 minutes to run.

## Re-evaluating the quality of the assembly {.tabset}
### BUSCO {-}
#### Short summary {-}
```
# BUSCO version is: 5.2.2 
# The lineage dataset is: actinopterygii_odb10 (Creation date: 2021-02-19, number of genomes: 26, number of BUSCOs: 3640)
# Summarized benchmarking in BUSCO notation for file /home/rccuser/shared/coley_files/trinity_supertran_fuscus.fasta
# BUSCO was run in mode: transcriptome

	***** Results: *****

	C:92.5%[S:90.0%,D:2.5%],F:3.2%,M:4.3%,n:3640	   
	3367	Complete BUSCOs (C)			   
	3277	Complete and single-copy BUSCOs (S)	   
	90	Complete and duplicated BUSCOs (D)	   
	117	Fragmented BUSCOs (F)			   
	156	Missing BUSCOs (M)			   
	3640	Total BUSCO groups searched		   

Dependencies and versions:
	hmmsearch: 3.1
	metaeuk: 6.a5d39d9
```

#### Graphing results {-}

<p float="center">

<img src="../imgs/busco_fuscus_super.png" style="width:650px;"/> 

</p>

### Examining RNA-Seq read representation (via Salmon) {-}
#### Mapping Rate {-}
|   Sample   |  Trinity Assembly  | SuperTranscript Assembly |
|:----------:|:------------------:|:------------------------:|
|   FUG10M2  |       99.8809%     |         96.3946%         |
|   FUG11F1  |       99.8791%     |         96.3215%         |
|   FUG11M2  |       99.8895%     |         96.2211%         |
|   FUG11M4  |       99.5617%     |         93.7851%         |
|   FUG12M1  |       99.8942%     |         96.3071%         |
|   FUG13F1  |       99.8912%     |         96.3934%         |
|   FUG13F4  |       99.9204%     |         96.4825%         |
|   FUG15M5  |        99.93%      |         96.5262%         |
|   FUG2F2   |       99.8933%     |         96.4757%         |
|   FUG3F2   |       99.8957%     |         96.4263%         |
|   FUL10M2  |       99.9229%     |         96.1121%         |
|   FUL11F1  |       99.935%      |         96.3962%         |
|   FUL11M2  |       99.9017%     |         96.771%          |
|   FUL11M4  |       99.9295%     |         96.8967%         |
|   FUL12M1  |       99.9099%     |         96.3373%         |
|   FUL13F1  |       99.9373%     |         96.2684%         |
|   FUL13F4  |       99.9419%     |         96.8745%         |
|   FUL15M5  |       99.9355%     |         96.7522%         |
|   FUL2F2   |       78.6675%     |         76.6031%         |
|   FUL3F2   |        99.94%      |         96.163%          |
|   FUO11F1  |       99.8901%     |         96.0597%         |
|   FUO13F1  |       99.905%      |         96.1805%         |
|   FUO13F4  |       99.9083%     |         96.1208%         |
|   FUO2F2   |       99.9399%     |         96.4821%         |
|   FUO3F2   |       99.832%      |         95.8775%         |
|   FUT10M2  |       99.9139%     |         95.9261%         |
|   FUT11M2  |       99.9051%     |         96.117%          |
|   FUT11M4  |       99.8352%     |         95.7566%         |
|   FUT12M1  |       99.872%      |         96.0704%         |
|   FUT15M5  |       99.8842%     |         96.1064%         |

# Prepping for differential expression analysis
## Importing transcript abundance with tximport
```{r create-sample-file}
##Create a file containing information about the samples
ID <- c("FUG10M2", "FUG11F1", "FUG11M2", "FUG11M4", "FUG12M1", "FUG13F1", "FUG13F4", "FUG15M5", "FUG2F2", "FUG3F2",
        "FUL10M2", "FUL11F1", "FUL11M2", "FUL11M4", "FUL12M1", "FUL13F1", "FUL13F4", "FUL15M5", "FUL2F2", "FUL3F2",
        "FUO11F1", "FUO13F1", "FUO13F4", "FUO2F2", "FUO3F2",
        "FUT10M2", "FUT11M2", "FUT11M4", "FUT12M1", "FUT15M5")
Sex <- c("M", "F", "M", "M", "M", "F", "F", "M", "F", "F",
         "M", "F", "M", "M", "M", "F", "F", "M", "F", "F",
         "F", "F", "F", "F", "F",
         "M", "M", "M", "M", "M")
Organ <- rep(c("Gill", "Liver", "Gonad"), times = c(10, 10, 10))
samples <- cbind(ID, Sex, Organ)
samples <- as.data.frame(samples)
write.table(samples, file = "FU_samples.txt", quote = FALSE, sep = "\t", col.names = TRUE, row.names = FALSE)

```

Generating the transcript abundance was done on the RCC via R command line:
```{r tximport, eval=FALSE}
#Enter R
conda activate R
R

#Load required package
library(tximport)

#Read in sample file
samples <- read.table("FU_samples.txt", header=TRUE)

#Create list of quant.sf files from salmon
files <- list.files("/home/rccuser/shared/coley_files/fuscus_super_salmon_quant/expression_files", full.names = TRUE)
names(files) <- paste0(samples$ID)
all(file.exists(files))
  ## [1] TRUE
  
#Pull the transcript-gene relationship from the .gtf file generated during the SuperTranscripts step
gtf <- read.table("trinity_genes.gtf", header = FALSE)
tx2gene <- gtf[,c(10, 10)] #Using the gene_id twice since there are no longer isoforms in the SuperTranscript assembly
tx2gene <- unique(tx2gene)
colnames(tx2gene) <- c("gene_id", "transcript_id")

#Building the matrix
txi.salmon.FU <- tximport(files, type = "salmon", tx2gene = tx2gene)
head(txi.salmon.FU$counts)
saveRDS(txi.salmon.FU, "txi.salmon_FU.RDS") #Move the file off the RCC to continue the Differential expression analysis in R

```
