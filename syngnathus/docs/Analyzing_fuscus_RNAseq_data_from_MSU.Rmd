---
title: "Analyzing *Syngnathus fuscus* RNAseq Data from MSU"
author: "Coley Tosto"
date: "`r Sys.Date()`"
output:
    html_document:
        code_folding: show
        toc: yes
        toc_float: yes
        number_sections: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r library, include = FALSE}

```

``` {r functions}

```

``` {r read-data}

```

# Pre-assembly Quality Control and Filtering
These reads were obtained from the **RTSF Genomics Core** at Michigan State University. They were downloaded with the File Transfer Protocol provided on their [website.](https://rtsf.natsci.msu.edu/genomics/data-retrieval/).

Quality scores were provided by MSU, so FastQC was not run on the raw reads. The provided scores were used to assist in choosing the settings for trimming and filtering. The trimming and filtering for these reads will follow the pipeline that was laid out for _Syngnathus floridae_. Any changes to scripts or steps will be highlighted in this document. Analysis was conducted in a Remote Computing Cluster (RCC) at the University of Canterbury.

For _Syngnathus fuscus_ we started with an average of 50 &pm; 6.9 million reads per sample for a total of 2.99 billion reads.

## Trimming the raw reads with Trimmomatic

Trimmomatic was install via a conda environment `Trim` on the RCC. `trimmomatic v0.39` was used for the following script.

```{bash run_trimmomatic, eval=FALSE}
#!/bin/bash

data=/home/rccuser/20220902_mRNASeq_PE150/ ##This is the location of the raw reads

for fq in ${data}*_R1.fastq.gz
        do
        base=$(basename $fq _R1.fastq.gz)
        echo "Running trimmomatic for ${base}..."
        time trimmomatic PE -threads 16 $fq ${data}${base}_R2.fastq.gz \
                ${data}trimmed/${base}_paired_R1.fastq.gz ${data}trimmed/${base}_unpaired_R1.fastq.gz \
                ${data}trimmed/${base}_paired_R2.fastq.gz ${data}trimmed/${base}_unpaired_R2.fastq.gz \
                ILLUMINACLIP:NexteraPE-PE.fa:2:30:10 HEADCROP:12 LEADING:3 TRAILING:10 SLIDINGWINDOW:4:15 MINLEN:50
done
```

-   Before running the script, change the `trimmomatic` line to `echo "..."` to make sure all of the variables are working correctly.

-   Then remove the `echo ""` and run the script as `nohup bash trim_script.sh > trim.out 2>&1 &`.\

-   The **NexteraPE-PE.fa** file was pulled from the [trimmomatic github](https://github.com/usadellab/Trimmomatic/tree/main/adapters) using `wget https://github.com/usadellab/Trimmomatic/blob/main/adapters/NexteraPE-PE.fa`

## Using Kraken2 to remove biological contamination

Kraken2 was installed in the conda environment `kraken2` on the RCC. `kraken2 v2.1.2` was used.

```{bash run_kraken2, eval = FALSE}
#!/bin/bash

#Create arguments
ref_fasta=$1 #desired reference database
input_dir=$2 #Input directory with the location on the reads
output_dir=$3 #Desired location for the output

## Loop through all pairs of reads in the input directory
for pair in $2/*_R1.fq.gz
        do

        #Extract the sample name from the file name
        sample=$(basename $pair _R1.fq.gz)

        ##Echo the sample name it is currently running
        echo "Running Kraken2 for ${sample}..."

        #Define the paths to the input and output files for this sample
        input1=$2/${sample}_R1.fq.gz
        input2=$2/${sample}_R2.fq.gz

        #Run Kraken2 on this pair of reads
        time kraken2 --threads 16 --db $1 --paired $input1 $input2 --unclassified-out $3/${sample}#.fq --report $3/${sample}.log

done
```

This script was run as `nohup bash bash_scripts/kraken2.sh ../kraken2_pluspfp/ fuscus_trimmed fuscus_nobio > kraken2.log 2>&1 &`

The kraken2 database used for this analysis included the standard database (archea, bacteria, viral) plus plant, fungi, and protozoan datbases. Only reads that did not map back to these databases were retained.

## Using SortMeRNA to remove rRNA contamination

SortmeRNA was installed via a conda environment `sortmerna` on the RCC.`sortmerna v4.3.6` was used

```{bash run_sortmeRNA, eval = FALSE}
#!/bin/bash

#Create arguments
input_dir=$1 #location of the reads
ref_fasta=$2 #desired reference fasta
output_dir_rrna=$3 #desired location for the reads that are rRNA
output_dir_norrna=$4 #desired location for the reads that are NOT rRNA

## Loop through all pairs of reads in the input directory
for pair in $1/*_R1.fastq.gz
        do

        #Extract the sample name from the file name
        sample=$(basename $pair _R1.fastq.gz)

        #Extract Fish ID
        ID=$(basename $pair _paired_R1.fastq.gz)

        ##Echo the sample name it is currently running
        echo "Running SortMeRNA for ${ID}..."

        #Define the paths to the input and output files for this sample
        input1=$1/${sample}_R1.fastq.gz
        input2=$1/${sample}_R2.fastq.gz

        #Run SortMeRNA on this pair of reads
        time sortmerna --threads 16 --ref $2 --reads $input1 --reads $input2 --fastx --aligned $3/${ID} --other $4/${ID} --out2

        #Remove SortMeRNA intermediate files before running again
        rm -r /home/rccuser/sortmerna/run/kvdb
done
```

This script was run as `nohup bash bash_scripts/sortmerna.sh fuscus_trimmed ../rRNA_databases_v4.3.4/smr_v4.3_fast_db.fasta fuscus_rrna fuscus_norrna > sortmerna.log 2>&1 &`.

The chosed reference FASTA file contains **a subset of sequences** from the default SortMeRNA database. Here, the number of sequences in each database is reduced to improve the speed of the analysis.

## Doing a k-mer based correction with RCorrector

The Rcorrector github repo was cloned and Rcorrector was installed in the /shared folder on the RCC.

```{bash run_Rcorrector, eval = FALSE}
#!/bin/bash

#Create arguments
rcorrector_path=$1 #Path to the run_rcorrector.pl file
input_dir=$2 #Path to the input directory containing the reads
output_dir=$2 #Path to the desired output location

##Loop through all pairs of reads in the directory
for pair in $2/*_R1.fq
        do

        #Extract sample name from the file name
        sample=$(basename $pair _R1.fq)

        #Echo the sample name that is currently running
        echo "Running rcorrector for ${sample} ..."

        #Run rcorrector on this pair of reads
        time perl $1/run_rcorrector.pl -t 16 -1 $2/${sample}_R1.fq -2 $2/${sample}_R2.fq -od $3
done
```

This script was run as `nohup bash bash_scripts/rcor.sh ../../rcorrector fuscus_nobio fuscus_kmer_corrected/ > rcor.log 2>&1 &`. After this step the fasta files were g-zipped.

# Checking quality of trimmed and filtered reads
The quality of the reads once they finished going through the filtering and trimming pipeline outlined above was assessed with FastQC and the results were compiled using MultiQC.

```{bash run_fastqc, eval=FALSE}
#!/bin/bash

input_dir=$1 #location of the reads
output_dir=$2 #name of the desired output directory

for fq in $1/*.gz
        do
        base=$(basename $fq)
        echo "Running fastqc for ${base} ..."
        time fastqc $fq -t 16 -o $2
done
```

This script was run as `nohup bash bash_scripts/fastqc_script.sh fuscus_kmer_corrected fuscus_FastQC > fastqc.log 2>&1 &`. Once finished MultiQC was run: `multiqc floridae_FastQC`.

From the General stats reported in the MultiQC report we can see that following this filtering/trimming process we end up with an average of 42 &pm; 6.2 million reads per sample for a total of 2.54 billion reads.

# De novo transcriptome assembly
For _Syngnathus fuscus_ I have chosen to use the De novo RNA-seq Assembly Pipeline (DRAP) to build the assembly. DRAP includes three modules:
  1. **runDrap** generates the Oases or Trinity assembly
  2. **runMeta** gathers all assemblies and merges them together to remove redundancy between sets.
  3. **runAssessment** generates assembly and alignment metrics that are collected in a report
  
`DRAP v1.92` was installed using the Docker image onto the RCC. After installation the docker image was pulled with `docker pull sigenae/drap`.

## Working through the pipeline
### runDrap
To see all of the argument options for `runDrap` I ran `sudo docker run --rm -v`pwd`:`pwd` sigenae/drap runDrap --help`

````{bash run_runDrap, eval=FALSE}
#!/bin/bash

#Generate list of reads(comma separated)
R1=$(ls FUG*R1*.fq.gz | perl -pe 's/\n/,\/home\/rccuser\/shared\/coley_files\/fuscus_kmer_corrected\//g' | perl -pe 's/,\/home\/rccuser\/shared\/coley_files\/fuscus_kmer_corrected\/$//g')
R2=$(ls FUG*R2*.fq.gz | perl -pe 's/\n/,\/home\/rccuser\/shared\/coley_files\/fuscus_kmer_corrected\//g' | perl -pe 's/,\/home\/rccuser\/shared\/coley_files\/fuscus_kmer_corrected\/$//g')

#Generate arguments
out_dir_name=$1

sudo docker run --rm -v`pwd`:`pwd` sigenae/drap runDrap --no-trim --outdir `pwd`/$1 \
                --R1 `pwd`/$R1 \
                --R2 `pwd`/$R2 \
                --dbg trinity --norm-mem 188 --dbg-mem 188


```

This script was run in a screen session in the RCC. To run this you must be in the folder that contains the desired reads (For the `$R1` and `$R2`). The script was then run as `bash ../bash_scripts/runDrap.sh DRAP_fuscus_gills_Sept2023`.

`runDRAP.sh` was run **separately** for all the different tissue types (gills, liver, ovaries, testes).

When it is running, `runDRAP` should give outputs that look similar to this:
```

```

## The Oyster River Protocol
### Installing the ORP using Docker
To work through the pipeline, the docker image was used following the instructions on their [website](https://oyster-river-protocol.readthedocs.io/en/latest/docker_install.html):

  1. Pull the image from DockerHub:
      `sudo docker pull macmaneslab/orp:2.3.3`

  2. Run the image:
```
      sudo docker run -it \
      --mount type=bind,source=/home/rccuser/shared,target=/home/orp/docker \
      macmaneslab/orp:2.3.3 bash
```
      
  3. Test the installation: \
```
      cd $HOME/Oyster_River_Protocol/sampledata

      conda activate orp

      $HOME/Oyster_River_Protocol/oyster.mk \
      STRAND=RF \
      TPM_FILT=1 \
      MEM=5 \
      CPU=4 \
      READ1=test.1.fq.gz \
      READ2=test.2.fq.gz \
      RUNOUT=test
```
Installation test was successful.

### Running the ORP
It appears the the ORP is unable to receive multiple files for the forward and reverse reads, however, I wanted to use all of the reads across my samples in order to build the assemblies. To get around this, all of the R1 and R2 reads across all the samples were concatenated into one file respectively.
```
cd fuscus_kmer_corrected/
cat *R1.fq.gz > FUall_R1.fq.gz
cat *R2.fq.gz > FUall_R2.fq.gz

#Check that it worked by looking at the size of the new files
ls -h -l
  -rw-rw-r--  1 rccuser rccuser  84G Oct  3 14:59 FUall_R1.fq.gz
  -rw-rw-r--  1 rccuser rccuser  87G Oct  3 15:17 FUall_R2.fq.gz

```


The `oyster.mk` file was used to run the entire ORP in one go. Before running the below script the orp environment was activated with `source activate orp`.

```{bash orp_script, eval=FALSE}
#!/bin/bash

#Create arguements
oyster_mk_file=$1 #FULL path to the oyster.mk file
read_1=$2
read_2=$3
output_name=$4

#Run oyster.mk
$oyster_mk_file TPM_FILT=1 \
        STRAND=RF \
        MEM=188 \
        CPU=16 \
        READ1=$read_1 \
        READ2=$read_2 \
        RUNOUT=$output_name \
        LINEAGE=actinopterygii_odb10

```

This whole process was done inside of a screen environment. After entering the docker (see above section **step 2**) the script was run as `bash $HOME/docker/coley_files/bash_scripts/orp_merged.sh $HOME/Oyster_River_Protocol/oyster.mk $HOME/docker/coley_files/fuscus_kmer_corrected/FUall_R1.fq.gz $HOME/docker/coley_files/fuscus_kmer_corrected/FUall_R2.fq.gz fuscus_orp`.